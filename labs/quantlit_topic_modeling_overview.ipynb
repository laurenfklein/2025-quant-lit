{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncd3h7t24IIk"
      },
      "source": [
        "# Topic Modeling ##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDwnUgyF4IIo"
      },
      "source": [
        "## What is Topic Modeling? ##\n",
        "\n",
        "What is topic modeling? At its most basic level, topic modeling is an automated method for extracting the themes, or \"topics,\" from large sets of documents--like newswires, or literary scholarship, or as we'll explore here, articles in the Emory Wheel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5yXIKaM4IIp"
      },
      "source": [
        "There are numerous kinds of topic models, but the most popular and widely-used kind is latent Dirichlet allocation (LDA). It's so popular, in fact, that \"LDA\" and \"topic model\" are sometimes used interchangeably, even though LDA is only one type.\n",
        "\n",
        "LDA math is pretty complicated. We're not going to get very deep into the math just yet (or maybe not ever, depending on the time). But first we are going to introduce two important concepts that will help us conceptually understand how LDA topic models work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbWLFjuo4IIq"
      },
      "source": [
        "### 1) LDA is an Unsupervised Algorithm\n",
        "Topic modeling is a kind of machine learning. Machine learning always sounds complicated, but it really just means that computer algorithms are performing tasks without being explicitly programmed to do so and that they are \"learning\" how to perform these tasks by being fed training data. In the field of machine learning, algorithms are typically split into two broad categories: supervised and unsupervised. These categories describe how the algorithms are \"trained\" or how they \"learn.\" LDA is an unsupervised algorithm.\n",
        "\n",
        "If an algorithm is supervised, that means a researcher is helping to guide it with some kind of information, like labels. For example, if you wanted to create an algorithm that could identify pictures of cats vs pictures of dogs, you could train it with a bunch of pictures of cats that were clearly labeled CAT and a bunch of pictures of dogs that were clearly labeled DOG. The algorithm would then be able to learn which features are specific to cats vs dogs because you explicitly told it: this is a picture of a cat; this is a picture of a dog.\n",
        "\n",
        "If an algorithm is unsupervised, that means a researcher does not train it with outside information. There are no labels. The algorithm just learns that pictures of cats are more similar to each other and pictures of dogs are more similar to each other. The algorithm doesn't really know that one cluster is cats and one cluster is dogs; it just knows that there are two distinct clusters.\n",
        "\n",
        "Because LDA is an unsupervised algorithm, we don't tell our topic model which words or topics to look for. We only tell the topic model how many topics (or clusters of words) that we want returned. The topic model doesn't know anything about Frida Kahlo, Nella Larsen, and Jackie Robinson. It doesn't know anything about art, literature, and sports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urwfLeA24IIr"
      },
      "source": [
        "### 2) LDA is a Probabilistic Model\n",
        "LDA fundamentally relies on statistics and probabilities. Rather than calculating precise and unchanging metrics about a given corpus, as we've done thus far, a topic model makes a series of very sophisticated guesses about the corpus. These guesses will change slightly every time we run the topic model. This is important to remember as we analyze, interpret, and make arguments based on our results. All of our results in this lesson will be probabilities, and they'll change slightly every time we re-run the topic model.\n",
        "\n",
        "When we tell the topic model that we want to extract 15 topics from the Emory Wheel, here's what the topic model does:\n",
        "\n",
        "The topic model starts off with a slightly silly, backwards assumption. The topic model assumes that every single one of the 4000-some-odd articles in the corpus was written by someone who exclusively drew their words from 15 mystery topics, or 15 clusters of words. To spin it in a slightly different way with a different medium, the topic model assumes that there was one master artist with 15 different paints on her palette, who created all the articles by dipping her brush into these 15 paints alone, applying and blending them onto each canvas in different proportions. The topic model is trying to discover the 15 mystery topics that created all the Wheel articles, as well as the mixture of these topics that makes up each individual article.\n",
        "\n",
        "The topic model begins by taking a completely wild guess about the 15 topics, but then it iterates through all the words in all the article and makes better and better guesses. If the word \"student\" keeps showing up with the words \"stress\" and \"exam,\" and if all three words keep showing up in the same kinds of article, then the topic model starts to suspect that these three words should belong to the same topic. If the word \"film\" keeps showing up with \"Atlanta\" and \"industry,\" then the topic model suspects that they should belong to the same topic, too. The topic model finally arrives at its best guesses for the 15 topics that most likely created all the Emory Wheel articles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbzON5TC4IIs"
      },
      "source": [
        "## LDA explained again in more abstract terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOzLxfr24IIs"
      },
      "source": [
        "Probabilistic topic models begin with an assumption and a definition.\n",
        "\n",
        "The assumption: all documents contain a mixture of different topics.\n",
        "\n",
        "The definition: a topic is a collection of words, each with a different probability of occurance in a particular document (or other chunk of text) discussing that topic.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyKTRQOP4IIt"
      },
      "source": [
        "Here's a nice illustration, created by Ted Underwood, that shows this assumed relatioship between topics and documents.\n",
        "\n",
        "![topics and docs](https://tedunderwood.files.wordpress.com/2012/04/shapeart.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ1pyg0F4IIu"
      },
      "source": [
        "Above we see an example of the basic assumption of topic modeling: one topic might contain many occurrences of “organize,” “committee,” “direct,” and “lead.” Another might contain a lot of “mercury” and “arsenic,” with a few occurrences of “lead.”\n",
        "\n",
        "The three documents are assumed to contain both topics in different proportions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ComFUWe84IIv"
      },
      "source": [
        "But here is the thing: we can’t directly observe topics. All we actually have are the documents that attest to their existence. So in other words:\n",
        "\n",
        "**Topic modeling is a way of extrapolating backward from a collection of documents to infer the topics that could have generated them.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVEt6xSm4IIy"
      },
      "source": [
        "### You're speaking in prose (I mean Baysian statistics)! ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61DQtmre4IIy"
      },
      "source": [
        "When you begin to train a topic model, the model can only make a wild guess as to which words belong to which topic, since it doesn't know the topics in advance.\n",
        "\n",
        "But over time, the model \"learns\" which words are more likely to be associated with which topics, and \"updates its priors.\"\n",
        "\n",
        "This is the fundamental idea of Bayes' Rule, which underlies Baysian statistics. Bayes's Rule (or Bayes's Law or Bayes' Theorum) provides us a way of incorporating prior belief into an assessment of the probability of something happening, or being true. It is often employed recursively--so, you register your preheld belief (or \"prior\"), run the experiment, and then update what you thought you knew.\n",
        "\n",
        "When we do that with topic modeling, two things happen:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t46c2IN4IIy"
      },
      "source": [
        "1) Words will gradually become more common in topics where they are already common. And also,\n",
        "\n",
        "2) Topics will become more common in documents where they are already common."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Wei24N4IIz"
      },
      "source": [
        "Thus our model will gradually become more consistent as topics focus on specific words and documents. But it can’t ever become perfectly consistent, because words and documents don’t line up in one-to-one fashion. So the tendency for topics to concentrate on particular words and documents will eventually be limited by the actual, messy distribution of words across documents.\n",
        "\n",
        "That’s how topic modeling works in practice. You assign words to topics randomly and then just keep improving the model, to make your guess more internally consistent, until the model reaches an equilibrium that is as consistent as the collection allows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuhEkvgE4IIz"
      },
      "source": [
        "For a slightly more in depth explanation of how LDA works, see [this video](https://vimeo.com/53080123). (Start around 5:35)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKHowVaG4IIz"
      },
      "source": [
        "### A brief historical / technical digression... ###\n",
        "\n",
        "Topic modeling began as a US military project in the in the 1990s. The goal was to automatically detect changes in newswire text so that governmental and military organizations could be alerted to emerging geopolitical events. (For more on this history, see [Binder](https://dhdebates.gc.cuny.edu/read/untitled/section/4b276a04-c110-4cba-b93d-4ded8fcfafc9#ch18).)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wuU2z6u4II0"
      },
      "source": [
        "In the early 2000s, a team of computer science researchers released [MALLET](http://mallet.cs.umass.edu/topics.php), short for **MA**chine **L**earning for **L**anguag**E** **T**oolkit. As the name suggests, MALLET is a software toolkit that enables a range of NLP techniques. Today, people mostly only use it for topic modeling, which it remains very very good at.\n",
        "\n",
        "With that said, MALLET is written in Java, which means that it's not ideal for working in Python and Colab notebooks. Maria Antoniak, whose \"Birth Stories\" paper we'll soon read for this class, has written a convenient Python package that allows you to use MALLET in a Colab notebook. Her package is called [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), and you should think about using that if you end up using topic modeling as one of your methods for your final project.\n",
        "\n",
        "For today, though, we'll be using the slightly less accurate but decidedly easier to install [gensim](https://radimrehurek.com/gensim/about.html), a native Python library for topic modeling tht was created in the early 2010s by a computer science PhD student, Radim Rehurek. Its ease of use has made the use of topic models explode--although, I should reiterate, most people end up returning to MALLET for research-level code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kwWNnOZ4II0"
      },
      "source": [
        "## Let's go! ##"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbcWL6FzFio5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d22d2560"
      },
      "source": [
        "%pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBJI4ePq4II0"
      },
      "outputs": [],
      "source": [
        "# import and setup modules we'll be using in this notebook\n",
        "import logging # for logging status etc\n",
        "import itertools # helpful library for iterating through things\n",
        "\n",
        "import numpy as np # this is a powerful python math package that many others are based on\n",
        "import gensim # our topic modeling library\n",
        "import os # for file i/o\n",
        "\n",
        "# configure logging, since topic modeling takes a while and it's good to know what's going on\n",
        "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
        "logging.root.level = logging.INFO\n",
        "\n",
        "# a helpful function that returns the first `n` elements of the stream as plain list.\n",
        "# we'll use this later\n",
        "def head(stream, n=10):\n",
        "    return list(itertools.islice(stream, n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V8tKME04II2"
      },
      "outputs": [],
      "source": [
        "# import some more gensim modules for processing the corpus\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load in our data\n",
        "\n",
        "For today's lesson, we'll be using a dataset of articles from the *Emory Wheel* betweeen 2014 and 2019. This dataset was created by Honggang Min and Kexin Guan for their final project in the 2019 iteration of QTM 340, and was generously transfered back to me for future class use.  \n",
        "\n",
        "The  documents are individual .txt files that are stored in a zip file on my Google Drive. Below is code similar that gets the data from my Google Drive and unzips it into a folder, which is further processed below."
      ],
      "metadata": {
        "id": "767jrRgH_ET-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading large files from Google Drive\n",
        "# https://github.com/wkentaro/gdown\n",
        "import gdown\n",
        "\n",
        "# then download the zip file\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1MbziyCKDr4FMFEDcC_b537I7j7ymnsUN', quiet=False)\n",
        "\n",
        "# unzip it\n",
        "!unzip wheel-cleaner.zip"
      ],
      "metadata": {
        "id": "_eGNrvJR_DIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the data\n",
        "\n",
        "There is always some sort of pre-processing involved in running any text analysis method. However, the particulars of this step are always a little different.\n",
        "\n",
        "To generate a topic model using gensim, each document needs to be fed into the model in the format: (title, tokens).\n",
        "\n",
        "(Tokens in this context means the specific unit that is processed. In this case, the tokens are just single words).\n",
        "\n",
        "The next few cells define a function to tokenize our documents, which we then to pre-process or *tokenize* our documents."
      ],
      "metadata": {
        "id": "qyHGXDno_ftb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdoZ1G9R4II2"
      },
      "source": [
        "### Step 1: Define our tokenizing function ###\n",
        "\n",
        "As previously discussed, gensim requires that we first tokenize our corpus.\n",
        "\n",
        "Here, we're going to define a own quick tokenizing function that makes use of gensim's [simple_preprocess function](https://radimrehurek.com/gensim/utils.html), which breaks a document into a list of lowercase tokens. The lower-casing is important for topic modeling since we want both uppercase and lowercase versions of the same word to be counted together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7W3O1ke4II3"
      },
      "outputs": [],
      "source": [
        "# here's some nice dense python for you:\n",
        "# this defines our tokenize function for future use\n",
        "def tokenize(text):\n",
        "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBG3s_Qr4II3"
      },
      "source": [
        "### Step 2: Process the docs ###\n",
        "\n",
        "Now that we have our tokenizing function, we can use it in the function below. This one iterates through each document (a newspaper article) our corpus (our Emory Wheel dataset) and returns each document in the format (title, tokens), as required.\n",
        "\n",
        "The gensim documentation tells us that we want to define a pre-processing function like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTEe2r3t4II3"
      },
      "outputs": [],
      "source": [
        "# A function to yield each doc in a base directory as a `(filename, tokens)` tuple.\n",
        "\n",
        "def iter_docs(base_dir):\n",
        "    docCount = 0\n",
        "    docs = os.listdir(base_dir)\n",
        "\n",
        "    for doc in docs:\n",
        "        if not doc.startswith('.'):\n",
        "            with open(base_dir + doc, \"r\") as file:\n",
        "                text = file.read()\n",
        "                tokens = tokenize(text)\n",
        "\n",
        "                yield doc, tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwREktmw4II4"
      },
      "source": [
        "The next step is to create a data structure that maps each word in the dataset to a numerical ID.\n",
        "\n",
        "This mapping step is required because most algorithms, including gensim's implementation of LDA, rely on numerical libraries that work with vectors indexed by numbers, not by words.\n",
        "\n",
        "The mapping can be constructed automatically, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tq-cvgQ4II5"
      },
      "outputs": [],
      "source": [
        "# set up the stream\n",
        "# this is the one line you'd change here with another corpus and/or corpus location\n",
        "stream = iter_docs('wheel-clean/')\n",
        "\n",
        "# all of the rest is standard from the gensim documentation\n",
        "doc_stream = (tokens for _, tokens in stream)\n",
        "\n",
        "id2word_wheel = gensim.corpora.Dictionary(doc_stream)\n",
        "\n",
        "print(id2word_wheel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5CpX9yJ4II5"
      },
      "source": [
        "The Dictionary (id2word_wheel) now contains all words that appeared in the corpus, along with an ID number for each of the words.\n",
        "\n",
        "Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyaYPgGi4II5"
      },
      "outputs": [],
      "source": [
        "id2word_wheel.token2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYAENQyH4II6"
      },
      "source": [
        "There aren't many things you need to do in order to tune your topic model, but one important thing do consider is whether you should filter the words.\n",
        "\n",
        "gensim also provides functions for this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qta1dq7V4II6"
      },
      "outputs": [],
      "source": [
        "# This line below would, for example, filter out 50 most frequent words.\n",
        "# It's commented out here because I don't want to use it in this case,\n",
        "# but it's handy to know about.\n",
        "# id2word_wheel.filter_n_most_frequent(50)\n",
        "\n",
        "# this line filters out words that appear only 1 doc, keeping the rest\n",
        "# I will use this one\n",
        "# Note how no_below and no_above take different data types. Not sure why!\n",
        "id2word_wheel.filter_extremes(no_below=2, no_above=1.0)\n",
        "\n",
        "id2word_wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAn7o87W4II7"
      },
      "source": [
        "Note that by removing the words that only appeared in a single document, we went from 77,892 unique words (or tokens) to 33,830. That's not a huge number for a topic model, but we'll see how it goes...\n",
        "\n",
        "Snce a streamed corpus and a dictionary is all we need to create the vectors for our topic model, we can get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K6KhIoU4II7"
      },
      "outputs": [],
      "source": [
        "# a class we need; this is the same for every topic model you create with gensim.\n",
        "# no need to modify it here\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
        "        self.dump_file = dump_file\n",
        "        self.dictionary = dictionary\n",
        "        self.clip_docs = clip_docs\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.titles = []\n",
        "        for title, tokens in itertools.islice(iter_docs(self.dump_file), self.clip_docs):\n",
        "            self.titles.append(title)\n",
        "            yield self.dictionary.doc2bow(tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.clip_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe30NDpK4II8"
      },
      "outputs": [],
      "source": [
        "# create a stream of bag-of-words vectors\n",
        "# here's another place where you'd change the name/location of the corpus if you want to\n",
        "# run a topic model on something else\n",
        "wheel_corpus = Corpus('wheel-clean/', id2word_wheel)\n",
        "\n",
        "# print the first vector in the stream to see what it looks like;\n",
        "# this is in the format (word_id, count in first doc)\n",
        "\n",
        "vector = next(iter(wheel_corpus))\n",
        "\n",
        "vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the vector above, which contains counts of the words that appear in the first document in the corpus, does not contain counts for all of the words in the corpus. Rather, it is what is known as a \"sparse\" vector, in that it omits all of the words in the corpus with a count of zero (that is, the words in the corpus that do not appear in each document).  "
      ],
      "metadata": {
        "id": "tnN7PXRf7_rn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiKCCRFq4II8"
      },
      "outputs": [],
      "source": [
        "# now we're ready to run our topic model!\n",
        "\n",
        "%time lda_model = gensim.models.LdaModel(wheel_corpus, num_topics=20, id2word=id2word_wheel, passes=50)\n",
        "\n",
        "# note that passes should be higher -- usually in the 50-100 range --\n",
        "# but in the interests of time we'll only do 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GRA45lS4II8"
      },
      "outputs": [],
      "source": [
        "# some additional helpful functions built into LdaModel\n",
        "\n",
        "# how to store corpus to LOCAL disk (for Colab users, see below)\n",
        "# from gensim.corpora import MmCorpus\n",
        "# MmCorpus.serialize('./wheel.corpus.mm', wheel_corpus)\n",
        "\n",
        "# how to store dictionary to disk\n",
        "# id2word_wheel.save('./wheel.dictionary')\n",
        "\n",
        "# how to store model to disk\n",
        "# lda_model.save('./lda_wheel-20topics_50iters.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8LBJuYs4II9"
      },
      "source": [
        "You can also load in a saved model. This is very helpful to know about, since generating new topic models takes time.\n",
        "\n",
        "Here, we're going to load in a (slightly) better topic model of the Emory Wheel with the same number of topics (15), but 50 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur4dz2Tc4II9"
      },
      "outputs": [],
      "source": [
        "# how you would load an old model from your own google drive\n",
        "# lda_model = gensim.models.LdaModel.load('/content/gdrive/My Drive/corpora/lda_wheel-20topics_5iters.model')\n",
        "\n",
        "# how we are going to all load in the same model -- from my google drive\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1ZnoYyOWfs4tBezNvMj3TTTa0ylXNoUJU', quiet=False)\n",
        "\n",
        "# unzip it\n",
        "# !unzip lda_wheel-20topics_50iters.zip\n",
        "\n",
        "# load it\n",
        "lda_model_50iters = gensim.models.LdaModel.load('lda_wheel-20topics_50iters.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSoRhvNv4II9"
      },
      "outputs": [],
      "source": [
        "# gensim comes with a bunch of functions that make interacting with the output\n",
        "# of the topic model a little easier. this one shows the topics.\n",
        "\n",
        "# first, let's be sure format the words nicely;\n",
        "import textwrap\n",
        "\n",
        "# this is the actual loading in of the topics\n",
        "topics = lda_model_50iters.show_topics(20, 20, formatted=False)\n",
        "\n",
        "# this is the print nicely part\n",
        "for topic in topics:\n",
        "    topic_num = topic[0]\n",
        "    topic_words = \"\"\n",
        "\n",
        "    topic_pairs = topic[1]\n",
        "    for pair in topic_pairs:\n",
        "        topic_words += pair[0] + \", \"\n",
        "\n",
        "    print(textwrap.fill(\"T\" + str(topic_num) + \": \" + topic_words) + \"\\n\", 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJkargqB4II9"
      },
      "source": [
        "## Topics and Labels\n",
        "\n",
        "Now you can see, perhaps, that we call a \"topic\" is really just a list of the most probable words for that topic, which are sorted in descending order of probability. The most probable word for the topic is the first word.\n",
        "\n",
        "Topic models start to get more powerful when we, as human researchers, analyze the most probable words for every topic and summarize what these words have in common. This summary can then be used as a descriptive label for the topic. Remember, since an LDA topic model is an unsupervised algorithm, it doesn't know what these words mean in relationship to one another. It's up to us, as the human researchers, to make meaning out of the topics.\n",
        "\n",
        "How might you label the following topics?\n",
        "\n",
        "✨Topic 7:✨\n",
        "\n",
        "```\n",
        "food, water, dining, emory, restaurant, meal, market, employees,\n",
        "options, menu, sodexo, cox, according, service, workers, chicken,\n",
        "hurricane, campus, waste, sustainability\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcohgCrN4II-"
      },
      "outputs": [],
      "source": [
        "# your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe-4iNrX4II-"
      },
      "source": [
        "✨Topic 8 :✨\n",
        "\n",
        "```\n",
        "sga, student, said, council, president, students, cc, college,\n",
        "vice, wheel, committee, legislature, government, year, graduate, spc,\n",
        "ma, executive, board, funding\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G_koq4KlRVjF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEmffI5y4II-"
      },
      "outputs": [],
      "source": [
        "# your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCtVPxXs4II-"
      },
      "source": [
        "✨Topic 0:✨\n",
        "\n",
        "```\n",
        "university, match, singles, won, set, team, win, doubles, matches,\n",
        "emory, williams, state, college, winning, court, championship, ncaa,\n",
        "sophomore, tennis, freshman\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8viwdakpRrYA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvQKTskj4II-"
      },
      "source": [
        "## Refining the topic model\n",
        "\n",
        "These are decent topics, but they're not amazing. Here are a few things you might want to try in order to fine-tune your model:\n",
        "\n",
        "* Filtering some of the most common words (see the filtering function above)\n",
        "* Generating more or fewer topics (we could try 10, for instance).\n",
        "\n",
        "Feel free to try those things on your own.\n",
        "\n",
        "But for the purposes of this class, let's take a bit of a closer look at the probabilities attached to each word in a single topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Yq_8Xp4II_"
      },
      "outputs": [],
      "source": [
        "# T0 looks coherent\n",
        "topic = topics[0]\n",
        "\n",
        "# the first item of the topic is the topic number\n",
        "topic_num = topic[0]\n",
        "\n",
        "print(\"For Topic 0, of all 30K+ words, these are the top words in the topic, sorted by proportion of the topic.\\n\")\n",
        "# the next item is another list with pairs of words and percentages\n",
        "topic_pairs = topic[1]\n",
        "for pair in topic_pairs:\n",
        "    print(pair[0] + \": \" + str(pair[1]))\n",
        "\n",
        "# since all topics contain all words, the sum of all of the percentages of each\n",
        "# topic should be 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WWPeqnu4II_"
      },
      "source": [
        "Let's flip it around and look at the topical composition of a single document.\n",
        "\n",
        "NB: MALLET provides this output automatically, but with gensim there's a bit more work required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpaGvxl14II_"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "tokens = []\n",
        "\n",
        "# open one article\n",
        "with open('wheel-clean/2014-10-02-Atlanta-Food-Truck-Park-Brings-Enriching-Epicurian-Experience.txt', \"r\") as file:\n",
        "    text = file.read()\n",
        "    print(textwrap.fill(text, 100))\n",
        "    tokens = tokenize(text) # remember this from above\n",
        "\n",
        "# create the bag of words for the document on the basis of the Wheel dictionary, created above\n",
        "doc_bow = id2word_wheel.doc2bow(tokens)\n",
        "\n",
        "# get the topics that the doc consists of\n",
        "# doc_topics = lda_model.get_document_topics(doc_bow)\n",
        "doc_topics = lda_model_50iters.get_document_topics(doc_bow)\n",
        "\n",
        "# sort doc_topics by percentage\n",
        "doc_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nArticle topics:\")\n",
        "\n",
        "# now we can cross-reference to find those topics and words\n",
        "for topic, prob in doc_topics:\n",
        "    print(\"T\" + str(topic) + \": \" + \"{:.2%}\".format(prob) + \" of document.\")\n",
        "\n",
        "        #  str(round(prob, 2)))\n",
        "\n",
        "    topic_words = \"Top words in topic: \"\n",
        "    select_topics = topics[topic]\n",
        "\n",
        "    for pair in select_topics[1]:\n",
        "        topic_words += pair[0] + \", \"\n",
        "\n",
        "    print(topic_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAYpyZsb4II_"
      },
      "source": [
        "### Evaluating Topics ###\n",
        "\n",
        "Gensim has several built-in methods for evaluating topics included as a model called [CoherenceModel](https://radimrehurek.com/gensim/models/coherencemodel.html). The fastest one to calculate is called u_mass, and in this case, the closer to zero (positive or negative), the better the score.\n",
        "\n",
        "Let's see how our model performs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVLPY4xY4II_"
      },
      "outputs": [],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "cm = CoherenceModel(model=lda_model, corpus=wheel_corpus, coherence='u_mass')\n",
        "# cm = CoherenceModel(model=lda_model_50iters, corpus=wheel_corpus, coherence='u_mass')\n",
        "\n",
        "\n",
        "coherence = cm.get_coherence()  # get coherence value\n",
        "\n",
        "coherence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCYc9bQh4IJA"
      },
      "source": [
        "Here's a review essay by Hanna Wallach et al. that summarizes a few methods of evaluation, including some involving humans in the loop: [\"Evaluation Methods for Topc Models\"](http://dirichlet.net/pdf/wallach09evaluation.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruY94ZYL4IJA"
      },
      "source": [
        "Another way to evalute topics is just to look at them.\n",
        "\n",
        "The pyLDAvis library lets you do this in a single line. It's very satisfying!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbVuzlFP4IJA"
      },
      "outputs": [],
      "source": [
        "# Install the pyLDAvis package if you haven't already\n",
        "!pip install pyLDAvis\n",
        "\n",
        "# LDA visualization tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis # Import the gensim_models submodule\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "# Check for and remove empty documents from the corpus\n",
        "wheel_corpus = [doc for doc in wheel_corpus if doc]\n",
        "\n",
        "# Use prepare function from the gensim_models submodule for Gensim models\n",
        "vis = gensimvis.prepare(lda_model, wheel_corpus, id2word_wheel)\n",
        "\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lauren Klein wrote this lesson in 2019 drawing on writing by [Ted Underwood](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/) and [Matthew Jockers](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/), this [video of a talk by David Mimno](https://vimeo.com/53080123), and [this notebook](https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html) by Radim Rehurek. It was supplemented in 2020 with additional materials from Dan Sinykin, and revised again in 2021, 2022, 2024, and 2025 and by Lauren Klein.*"
      ],
      "metadata": {
        "id": "Mj0RRCQZ4P6D"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}